<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Inference-Time Scaling of Diffusion Models</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="style.css" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>
  <main>
    <header>
      <h1>Inference-Time Scaling of Diffusion Models through Classical Search</h1>
      <a href="https://github.com/XiangchengZhang/Diffusion-inference-scaling" target="_blank" class="github-btn" style="display: inline-flex; align-items: center; gap: 8px; margin: 18px 0 0 10px; text-decoration: none; background: #24292f; color: white; padding: 6px 12px; border-radius: 5px; font-size: 1em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 0 16 16" width="20" fill="white">
          <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38
                   0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13
                   -.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07
                   -.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08
                   -.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09
                   2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82
                   2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01
                   2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
        </svg>
        GitHub
      </a>
      <div class="author-list">
        <span>Xiangcheng Zhang</span>
        <span>Haowei Lin</span>
        <span>Haotian Ye</span>
        <span>James Zou</span>
        <span>Jianzhu Ma</span>
        <span>Yitao Liang</span>
        <span>Yilun Du</span>
      </div>
      <div class="correspondence">
        Corresponding author: Yilun Du. Contact: <a href="mailto:ydu@seas.harvard.edu">ydu@seas.harvard.edu</a>
      </div>
    </header>
    <div class="content">
      <section>
        <h2>Abstract</h2>
        <p>Starting from the foundamental principles from classical search methods, such as hill climbing and graph search, we propose a 
          principled and general framework for inference-time scaling of diffusion models through search. Our framework consists of the following components:
          <ul>
            <li>Gradient-driven local search via annealed Langevin MCMC</li>
            <li>Tree-based global exploration via BFS/DFS</li>
          </ul>
          We carry out extensive experiments on various tasks, including long horizon maze planning, offline reinforcement learning and image generation, showing that classical search principles could serve as the foundation for inference-time scaling of diffusion models.
        </p>

      </section>

      <section>
        <h2>Introduction</h2>
        <p>Diffusion models have demonstrated exceptional performance in generative modeling for continous domains such as images, videos and robotics. However, they still struggles with 
          flexible control at inference time, which is crucial for tasks like planning and reinforcement learning. We approach this challenge from the inference-time scaling perspective, leveraging classical search methods to 
          search the generative space of diffusion models for high quality samples.
        </p>
        <figure>
          <img src="teaser.png" alt="Illustration of our search framework" />
          <figcaption>Illustration of our search framework. </figcaption>
        </figure>
        <p>
          As shown in the above figure, we first search the vicinity of the samples via gradient-guided local search. 
          To avoid being stuck in local maximums and OOD samples, 
          we explore the the diverse modes in the multimodal generative space globally, 
          improving the effciency using tree-based search methods such as BFS and DFS. 
          This unfied search framework allows us to efficiently sample from global optimal modes with high quality samples.
        </p>
      </section>

      <section>
        <h2>Local Search</h2>
        <p>We fomulate the problem as sampling from the composed distribution of the base distribution \({p}_0(\mathbf{x})\) and the verifier score
          \(f(\mathbf{x})\):
        </p>
        <p>
          $$ \tilde{p}_0(\mathbf{x}_0) \propto p_0(\mathbf{x}_0) f(\mathbf{x}_0)^\lambda $$
        </p>
        <p>
          Through annealed Langevin MCMC, we construct a sequence of distributions \(\tilde{q}_t(\mathbf{x}_t)\) that gradually approaches \(\tilde{p}_0(\mathbf{x}_0)\): 
          $$ \tilde{q}_t(\mathbf{x}_t) \propto  q_t(\mathbf{x}_t) \tilde{f}_t(\mathbf{x}_t) $$
          where \(q_t(\mathbf{x}_t)\) is the distribution of the diffusion model at time step \(t\), and \(\tilde{f}_t(\mathbf{x}_t)\) is constructed using \(f(\mathbf{x}_{0|t})\).
          We then sample from the distribution \(\tilde{q}_t(\mathbf{x}_t)\) at each step \(t\) using Langevin dynamics:
          $$ \mathbf{x}_{t}^{i+1} = \mathbf{x}_t^i + \frac{\epsilon}{2} \nabla_{\mathbf{x}_t} \log \tilde{q}_t(\mathbf{x}_t^i) + \sqrt{\epsilon} \mathbf{z}_t^i\,,~~~\mathbf{z}_t^i\sim\mathcal{N}(\mathbf{0},\mathbf{I}) $$
          which can be seen as conducting hill-climbing, following the gradient flow of the KL divergence between \(\tilde{q}_t\) and the distribution of current sample \(\mathbf{x}_t\).
        </p>
        
      </section>

      <section>
        <h2>Global Search</h2>
        <p>To explore diverse modes in the multimodal distribution induced by diffusion models, we view the sampling process as a search tree, and use classical search methods such as 
          breadth-first search (<strong>BFS</strong>) and depth-first search (<strong>DFS</strong>) to explore the space efficiently.
        </p>
         <p>
          This fomulation encompasses other search methods as <strong>instances or combinations of BFS and DFS</strong>, 
          and through our extensive experiments, 
          we demonstrate the superior efficiency and adaptivity of our search framework compared to existing methods.
        </p>
         <figure>
          <img src="treesearch.png" alt="Search tree for sampling" />
          <figcaption>Illustration of global tree search methods</figcaption>
        </figure>
        <h3>Best-of-N</h3>
        <p>
          As a baseline in global search, in <strong>Best-of-N</strong>, we sample N samples from the diffusion model in paralle,
          and select the best sample according to the verifier score \(f(\mathbf{x})\). 
          This method is simple and effective, but it does not utilize information from intermediate states, thus being inefficient.
        </p>
        <h3>BFS</h3>
        <p>
          In BFS, similar to best-first search where we use a heuristic to evaluate intermediate states, 
          we dynamically allocate compute to intermediate particles \(\mathbf{x}_t\) at certain time step \(t\) based on their verifier scores \(f(\mathbf{x}_{0|t})\). 
        </p>
        <p>
          In <strong>BFS-Resampling</strong>, we sample \(n_t^k\) children for particle \(\mathbf{x}_t^k\) at time step \(t\) propotional to the verifier score:
          $$ n_t^k =  \texttt{Round} \left(N_t\frac{f(\mathbf{x}_{0|t}^k)}{\sum_{j=1}^{N_t} f(\mathbf{x}_{0|t}^j)} \right) $$
          where \(N_t\) is the number of particles at time step \(t\).
        </p>
        <p>
          In <strong>BFS-Pruning</strong>, when using determinstic ODE-solvers as samplers, sampling more than one particle results in duplication. 
          We then prune the particles with:
          $$n_t^k = \min\left(1, \texttt{Round} \left(N_t\frac{f(\mathbf{x}_{0|t}^k)}{\sum_{j=1}^{N_t} f(\mathbf{x}_{0|t}^j)} \right) \right)$$
          so we sample at most one children for each parent particle \(\mathbf{x}_t^k\).
        </p>
        <h3>DFS</h3>
        In <strong>DFS</strong>, we only denoise one particle, and set a threshold \(\delta_t\) for the verifier score. 
        When the verifier score estimate falls below the threshold:
        $$ f(\mathbf{x}_{0|t}) < \delta_t $$
        we <strong>backtrack</strong> by adding noise to a higher noise level:
        $$ t_{\text{next}}=t+\Delta,~~~\mathbf{x}_{t_{\text{next}}} = x_t + \sqrt{\sigma_{t_{\text{next}}}^2-\sigma_t^2}\mathbf{z}\,,~~~\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$$
        where \(\Delta\) is the backtrack step size, and \(q\) is the forward diffusion process.
        
      </section>

      <section>
        <h2>Experiments</h2>
        <p>We conduct extensive experiments on various tasks, including long horizon maze planning, offline RL and image generation</p>
        <h3>Maze Planning</h3>
        <p>
        We show that with inference-scaling, diffusion models can succeed in long horizon planning in complex maze environments. 
        We  define the verifier loss as the collision of particles in the generated trajectory with the maze wall blocks:
        $$L(\mathbf{\tau})=\sum_{i=1}^H\texttt{check_collision}\left(\text{point}_i\right)\texttt{distance}\left(\text{point}_i\,,~ \text{maze walls}\right)$$
        so we minimize the violation of the trajectory with inference-scaling.
        </p>
        <div class="fig-row" style="align-items: flex-start;">
          <figure>
            <img src="maze_giant.png" alt="maze_giant"/>
            <figcaption>Generated plan for PointMaze Giant</figcaption>
          </figure>
          <figure>
            <img src="maze_ultra.png" alt="maze_ultra"/>
            <figcaption>Generated plan for PointMaze Ultra</figcaption>
          </figure>
        </div>
        <h3>Offline RL</h3>
        <p>
        We use a pretrained diffusion model and a pretrained Q-function as verifier for offline RL. 
        We formulate offline RL as sampling from the Q-weighted distribution:
        $$ \tilde{\pi}(\mathbf{a}_0|\mathbf{s}) \propto \pi(\mathbf{a}_0|\mathbf{s}) \exp\left(\lambda Q(\mathbf{a}_0,\mathbf{s})\right) $$
        where \(\pi\) is the pretrained diffusion model, and \(Q\) is the pretrained Q-function. 
        </p>
        <p>
        We evaluate our method on D4RL locomotion tasks. 
        Our training-free method can achieve comparable performance to the state-of-the-art training-required methods.
        </p>
        <div class="table-wrapper">
          <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%; font-family: sans-serif;">
    <caption style="caption-side: top; font-weight: bold; text-align: left; padding: 10px 0;">
      Performance on D4RL locomotion tasks (mean ± std across 5 trials). Bold values are within 5% of the best for each task.
    </caption>
    <thead style="background-color: #f0f0f0;">
      <tr>
        <th>Dataset</th>
        <th>Environment</th>
        <th>CQL</th>
        <th>BCQ</th>
        <th>IQL</th>
        <th>SfBC</th>
        <th>DD</th>
        <th>Diffuser</th>
        <th>D-QL</th>
        <th>QGPO</th>
        <th>TTS (ours)</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>Medium-Expert</td><td>HalfCheetah</td><td>62.4</td><td>64.7</td><td>86.7</td><td><b>92.6</b></td><td>90.6</td><td>79.8</td><td><b>96.1</b></td><td><b>93.5</b></td><td><b>93.9 ± 0.3</b></td></tr>
      <tr><td>Medium-Expert</td><td>Hopper</td><td>98.7</td><td>100.9</td><td>91.5</td><td>108.6</td><td><b>111.8</b></td><td><b>107.2</b></td><td><b>110.7</b></td><td><b>108.0</b></td><td>104.4 ± 3.1</td></tr>
      <tr><td>Medium-Expert</td><td>Walker2d</td><td><b>111.0</b></td><td>57.5</td><td><b>109.6</b></td><td><b>109.8</b></td><td><b>108.8</b></td><td><b>108.4</b></td><td><b>109.7</b></td><td><b>110.7</b></td><td><b>111.4 ± 0.1</b></td></tr>
      <tr><td>Medium</td><td>HalfCheetah</td><td>44.4</td><td>40.7</td><td>47.4</td><td>45.9</td><td>49.1</td><td>44.2</td><td>50.6</td><td><b>54.1</b></td><td><b>54.8 ± 0.1</b></td></tr>
      <tr><td>Medium</td><td>Hopper</td><td>58.0</td><td>54.5</td><td>66.3</td><td>57.1</td><td>79.3</td><td>58.5</td><td>82.4</td><td><b>98.0</b></td><td><b>99.5 ± 1.7</b></td></tr>
      <tr><td>Medium</td><td>Walker2d</td><td>79.2</td><td>53.1</td><td>78.3</td><td>77.9</td><td><b>82.5</b></td><td>79.7</td><td><b>85.1</b></td><td><b>86.0</b></td><td><b>86.5 ± 0.2</b></td></tr>
      <tr><td>Medium-Replay</td><td>HalfCheetah</td><td><b>46.2</b></td><td>38.2</td><td>44.2</td><td>37.1</td><td>39.3</td><td>42.2</td><td><b>47.5</b></td><td><b>47.6</b></td><td><b>47.8 ± 0.4</b></td></tr>
      <tr><td>Medium-Replay</td><td>Hopper</td><td>48.6</td><td>33.1</td><td>94.7</td><td>86.2</td><td><b>100.0</b></td><td><b>96.8</b></td><td><b>100.7</b></td><td><b>96.9</b></td><td><b>97.4 ± 4.0</b></td></tr>
      <tr><td>Medium-Replay</td><td>Walker2d</td><td>26.7</td><td>15.0</td><td>73.9</td><td>65.1</td><td>75.0</td><td>61.2</td><td><b>94.3</b></td><td>84.4</td><td>79.3 ± 9.7</td></tr>
      <tr><td colspan="2"><b>Average (Locomotion)</b></td><td>63.9</td><td>51.9</td><td>76.9</td><td>75.6</td><td>81.8</td><td>75.3</td><td>86.3</td><td><b>86.6</b></td><td>86.1</td></tr>
    </tbody>
  </table>
        </div>
        <h3>Image Generation</h3>
        <p>
      We evaluate our method on compositional text-to-image generation on <a href="https://github.com/Karine-Huang/T2I-CompBench">CompBench</a>, and class conditional ImageNet generation using an unconditional diffusion model with a pretrained classifier.
    </p>
    <p>
      In compositional test-to-image, we show that inference-time search against a pretrained visual verifier such as <a href="https://github.com/xingyizhou/UniDet">UniDet</a> and <a href="https://github.com/salesforce/BLIP">BLIP</a> can generate images with complex compositional prompts. 
    </p>
    <p>
      Below show the results of prompt "eight bottles" with and without inference scaling, which requires the model to generate the correct number of objects.
      We use a <strong>UniDet Model</strong> to count the number of objects in the generated image.
      After inference scaling the number of bottles in the generated image follows the prompt, while without inference scaling the number of bottles is not consistent with the prompt.
    </p>
    <div class="fig-row">
      <figure>
        <img src="t2i1.png" alt="wrong sample without inference scaling" />
        <figcaption>Generated sample without inference scaling. The number of bottles does not follow the prompt "eight bottles"</figcaption>
      </figure>
      <figure>
        <img src="t2i2.png" alt="correct sample with inference scaling" />
        <figcaption>Generated sample with inference scaling using UniDet. The number of bottles follows the prompt "eight bottles"</figcaption>
      </figure>
    </div>

      <p>
        In class-conditional image generation, we generate samples aligned with a target class label using an <strong>unconditional diffusion model</strong> guided by a pretrained ViT <strong>classifier</strong> as a verifier.
        $$\tilde{p}_0(\mathbf{x}_0) \propto p_0(\mathbf{x}_0) p\left(c|\mathbf{x}_0\right)$$
        Below, we show the uncurated samples for class 222 (“kuvasz”) on ImageNet, demonstrating diverse samples that accurately reflect the class label.
      </p>
      <figure>
        <img src="imagenet.png" alt="ImageNet class conditional generation", width="60%"/>
        <figcaption>
          Uncurated samples after inference-time scaling on ImageNet, with class 222 "kuvasz" 
      </figcaption>
      </figure>
        

      </section>
      <section>
        <h2>Conclusion</h2>
        <p>We propose a principled and general framework for inference-time scaling of diffusion models through search, which can be applied to various tasks such as planning, reinforcement learning and image generation. Still, inference-time base approaches require more hyper-parameter tuning than naive sampling, which we leave as a future work</p>
      </section>
    </div>
  </main>
</body>
</html>