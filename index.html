<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Inference-Time Scaling of Diffusion Models</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="style.css" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>
  <main>
    <header>
      <h1>Inference-Time Scaling of Diffusion Models through Classical Search</h1>
      <a href="https://github.com/XiangchengZhang/Diffusion-inference-scaling" target="_blank" class="github-btn" style="display: inline-flex; align-items: center; gap: 8px; margin: 18px 0 0 10px; text-decoration: none; background: #24292f; color: white; padding: 6px 12px; border-radius: 5px; font-size: 1em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 0 16 16" width="20" fill="white">
          <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38
                   0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13
                   -.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07
                   -.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08
                   -.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09
                   2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82
                   2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01
                   2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
        </svg>
        GitHub
      </a>
      <div class="author-list">
        <span>Xiangcheng Zhang<sup>1</sup></span>
        <span>Haowei Lin<sup>1</sup></span>
        <span>Haotian Ye<sup>2</sup></span>
        <span>James Zou<sup>2</sup></span>
        <span>Jianzhu Ma<sup>1</sup></span>
        <span>Yitao Liang<sup>1</sup></span>
        <span>Yilun Du<sup>3</sup><sup>&dagger;</sup></span>
      </div>
      <div class="affiliations">
        <span><sup>1</sup>Helixon US Inc.</span>
        <span><sup>2</sup>Stanford University</span>
        <span><sup>3</sup>Harvard University</span>
      </div>
      <div class="correspondence">
        <sup>&dagger;</sup>Corresponding author: Yilun Du. Contact: <a href="mailto:ydu@seas.harvard.edu">ydu@seas.harvard.edu</a>
      </div>
    </header>
    <div class="content">
      <section>
        <h2>Abstract</h2>
        <p>Starting from the foundamental principles from classical search methods, such as hill climbing and graph search, we propose a 
          principled and general framework for inference-time scaling of diffusion models through search. Our framework consists of the following components:
        </p>
          <ul>
            <li>Gradient-driven local search via annealed Langevin MCMC</li>
            <li>Tree-based global exploration via BFS/DFS</li>
          </ul>
        <p>
          We carry out extensive experiments on various tasks, including long horizon maze planning, offline reinforcement learning and image generation, showing that classical search principles could serve as the foundation for inference-time scaling of diffusion models.
        </p>

      </section>

      <section>
        <h2>Introduction</h2>
        <p>Diffusion models have demonstrated exceptional performance in generative modeling for continous domains such as images, videos and robotics. However, they still struggles with 
          flexible control at inference time, which is crucial for tasks like planning and reinforcement learning. We approach this challenge from the inference-time scaling perspective, leveraging classical search methods to 
          search the generative space of diffusion models for high quality samples.
        </p>
        <div style="display: flex; justify-content: center;">
          <video controls loop muted style="width: 60%; max-width: 600px; margin: 20px auto; align-items: center;" poster="teser-video.png">
            <source src="teser-video.mp4" type="video/mp4">
          </video>
        </div>
        <p>
          As shown in the above demo, we first search the vicinity of the samples via gradient-guided local search. 
          To avoid being stuck in local maximums and OOD samples, 
          we explore the the diverse modes in the multimodal generative space globally, 
          improving the effciency using tree-based search methods such as BFS and DFS. 
          This unfied search framework allows us to efficiently sample from global optimal modes with high quality samples.
        </p>
      </section>

      <section>
        <h2>Local Search</h2>
        <p>We fomulate the problem as sampling from the composed distribution of the base distribution \({p}_0(\mathbf{x})\) and the verifier score
          \(f(\mathbf{x})\):
        </p>
        <p>
          $$ \tilde{p}_0(\mathbf{x}_0) \propto p_0(\mathbf{x}_0) f(\mathbf{x}_0)^\lambda $$
        </p>
        <p>
          Through annealed Langevin MCMC, we construct a sequence of distributions \(\tilde{q}_t(\mathbf{x}_t)\) that gradually approaches \(\tilde{p}_0(\mathbf{x}_0)\): 
          $$ \tilde{q}_t(\mathbf{x}_t) \propto  q_t(\mathbf{x}_t) \tilde{f}_t(\mathbf{x}_t) $$
          where \(q_t(\mathbf{x}_t)\) is the distribution of the diffusion model at time step \(t\), and \(\tilde{f}_t(\mathbf{x}_t)\) is constructed using \(f(\mathbf{x}_{0|t})\).
          We then sample from the distribution \(\tilde{q}_t(\mathbf{x}_t)\) at each step \(t\) using Langevin dynamics:
          $$ \mathbf{x}_{t}^{i+1} = \mathbf{x}_t^i + \frac{\epsilon}{2} \nabla_{\mathbf{x}_t} \log \tilde{q}_t(\mathbf{x}_t^i) + \sqrt{\epsilon} \mathbf{z}_t^i\,,~~~\mathbf{z}_t^i\sim\mathcal{N}(\mathbf{0},\mathbf{I}) $$
          which can be seen as conducting hill-climbing, following the gradient flow of the KL divergence between \(\tilde{q}_t\) and the distribution of current sample \(\mathbf{x}_t\).
        </p>
        <div style="display: flex; justify-content: center;">
          <video controls loop muted style="width: 60%; max-width: 600px; margin: 20px auto; align-items: center;" poster="local_search_video.png">
            <source src="local_search_video.mp4" type="video/mp4">
          </video>
        </div>
        
      </section>

      <section>
        <h2>Global Search</h2>
        <p>To explore diverse modes in the multimodal distribution induced by diffusion models, we view the sampling process as a search tree, and use classical search methods such as 
          breadth-first search (<strong>BFS</strong>) and depth-first search (<strong>DFS</strong>) to explore the space efficiently.
        </p>
         <p>
          This fomulation encompasses other search methods as <strong>instances or combinations of BFS and DFS</strong>, 
          and through our extensive experiments, 
          we demonstrate the superior efficiency and adaptivity of our search framework compared to existing methods.
        </p>
         <figure>
          <img src="treesearch.png" alt="Search tree for sampling" />
          <figcaption>Illustration of global tree search methods</figcaption>
        </figure>
        <h3>Best-of-N</h3>
        <p>
          As a baseline in global search, in <strong>Best-of-N</strong>, we sample N samples from the diffusion model in paralle,
          and select the best sample according to the verifier score \(f(\mathbf{x})\). 
          This method is simple and effective, but it does not utilize information from intermediate states, thus being inefficient.
        </p>
        <h3>BFS</h3>
        <p>
          In BFS, similar to best-first search where we use a heuristic to evaluate intermediate states, 
          we dynamically allocate compute to intermediate particles \(\mathbf{x}_t\) by sampling varying number of children based on their verifier score estimate.
        </p>
        <p>
          In <strong>BFS-Resampling</strong>, we sample \(n_t^k\) children for particle \(\mathbf{x}_t^k\) at time step \(t\) propotional to their verifier score:
          $$ n_t^k =  \texttt{Round} \left(N_t\frac{f(\mathbf{x}_{0|t}^k)}{\sum_{j=1}^{N_t} f(\mathbf{x}_{0|t}^j)} \right) $$
          where \(N_t\) is the number of particles at time step \(t\).
        </p>
        <div style="display: flex; justify-content: center;">
          <video controls loop muted style="width: 50%; max-width: 600px; margin: 20px auto; align-items: center;" poster="BFS-resample-video.png">
            <source src="BFS-resample-video.mp4" type="video/mp4">
          </video>
        </div>
        <p>
          When using determinstic ODE-solvers as samplers, sampling more than one children from the same particle results in duplication. 
          In <strong>BFS-Pruning</strong>, we then prune the particles with:
          $$n_t^k = \min\left(1, \texttt{Round} \left(N_t\frac{f(\mathbf{x}_{0|t}^k)}{\sum_{j=1}^{N_t} f(\mathbf{x}_{0|t}^j)} \right) \right)$$
          so we sample at most one children for each parent particle \(\mathbf{x}_t^k\), gradually removing bad samples.
        </p>
        <div style="display: flex; justify-content: center;">
          <video controls loop muted style="width: 50%; max-width: 600px; margin: 20px auto; align-items: center;" poster="BFS-pruning-video.png">
            <source src="BFS-pruning-video.mp4" type="video/mp4">
          </video>
        </div>
        <h3>DFS</h3>
        <p>
        In <strong>DFS</strong>, we only denoise one particle, and set a threshold \(\delta_t\) for the verifier score. 
        When the verifier score estimate falls below the threshold:
        $$ f(\mathbf{x}_{0|t}) < \delta_t $$
        we <strong>backtrack</strong> by adding noise to a higher noise level:
        $$ t_{\text{next}}=t+\Delta,~~~\mathbf{x}_{t_{\text{next}}} = \mathbf{x}_t + \sqrt{\sigma_{t_{\text{next}}}^2-\sigma_t^2}\mathbf{z}\,,~~~\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$$
        where \(\Delta\) is the backtrack step size.
        </p>
        <div style="display: flex; justify-content: center;">
          <video controls loop muted style="width: 50%; max-width: 600px; margin: 20px auto; align-items: center;" poster="DFS-video.png">
            <source src="DFS-video.mp4" type="video/mp4">
          </video>
        </div>
      </section>

      <section>
        <h2>Experiments</h2>
        <p>We conduct extensive experiments on various tasks, including long horizon maze planning, offline RL and image generation</p>
        <h3>Maze Planning</h3>
        <p>
        We show that with inference-scaling, diffusion models can succeed in long horizon planning in complex maze environments. 
        We sample from the composed distribution:
        $$ \tilde{p}_0(\mathbf{\tau}_0) \propto p_0(\mathbf{\tau}_0) \exp{\left(-\lambda L(\mathbf{\tau}_0)\right)}$$
        where \( L(\mathbf{\tau}_0) \) measures the collision of the trajectory \(\mathbf{\tau}_0\) with the maze walls, 
        so we minimize the violation of the generated plan.
        </p>
        <p>
          Below, we show with local search, the model is able to generate complex and successful plans.
        </p>
        <div class="fig-row" style="align-items: flex-start;">
          <figure>
            <img src="maze_giant.png" alt="maze_giant"/>
            <figcaption>Generated plan for PointMaze Giant</figcaption>
          </figure>
          <figure>
            <img src="maze_ultra.png" alt="maze_ultra"/>
            <figcaption>Generated plan for PointMaze Ultra</figcaption>
          </figure>
        </div>
        <p>
          With global search, we can further reach over 80% success rate.
        </p>
        <div class="fig-row" style="align-items: flex-start;">
          <figure>
            <img src="maze_giant_curve.png" alt="maze_giant"/>
            <figcaption>Scaling curve for PointMaze Giant</figcaption>
          </figure>
          <figure>
            <img src="maze_ultra_curve.png" alt="maze_ultra"/>
            <figcaption>Scaling curve for PointMaze Ultra</figcaption>
          </figure>
        </div>

        <h3>Offline RL</h3>
        <p>
        We use a pretrained diffusion model and a pretrained Q-function as verifier for offline RL. 
        We formulate offline RL as sampling from the Q-weighted distribution:
        $$ \tilde{\pi}(\mathbf{a}_0|\mathbf{s}) \propto \pi(\mathbf{a}_0|\mathbf{s}) \exp\left(\lambda Q(\mathbf{a}_0,\mathbf{s})\right) $$
        where \(\pi\) is the pretrained diffusion model, and \(Q\) is the pretrained Q-function. 
        </p>
        <p>
        We evaluate our method on D4RL locomotion tasks. 
        Our training-free method can achieve comparable performance to the state-of-the-art training-required methods.
        </p>
        <div class="table-wrapper">
          <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%; font-family: sans-serif;">
    <caption style="caption-side: top; font-weight: bold; text-align: left; padding: 10px 0;">
      Performance on D4RL locomotion tasks (mean ± std across 5 trials). Bold values are within 5% of the best for each task.
    </caption>
    <thead style="background-color: #f0f0f0;">
      <tr>
        <th>Dataset</th>
        <th>Environment</th>
        <th>CQL</th>
        <th>BCQ</th>
        <th>IQL</th>
        <th>SfBC</th>
        <th>DD</th>
        <th>Diffuser</th>
        <th>D-QL</th>
        <th>QGPO</th>
        <th>TTS (ours)</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>Medium-Expert</td><td>HalfCheetah</td><td>62.4</td><td>64.7</td><td>86.7</td><td><b>92.6</b></td><td>90.6</td><td>79.8</td><td><b>96.1</b></td><td><b>93.5</b></td><td><b>93.9 ± 0.3</b></td></tr>
      <tr><td>Medium-Expert</td><td>Hopper</td><td>98.7</td><td>100.9</td><td>91.5</td><td>108.6</td><td><b>111.8</b></td><td><b>107.2</b></td><td><b>110.7</b></td><td><b>108.0</b></td><td>104.4 ± 3.1</td></tr>
      <tr><td>Medium-Expert</td><td>Walker2d</td><td><b>111.0</b></td><td>57.5</td><td><b>109.6</b></td><td><b>109.8</b></td><td><b>108.8</b></td><td><b>108.4</b></td><td><b>109.7</b></td><td><b>110.7</b></td><td><b>111.4 ± 0.1</b></td></tr>
      <tr><td>Medium</td><td>HalfCheetah</td><td>44.4</td><td>40.7</td><td>47.4</td><td>45.9</td><td>49.1</td><td>44.2</td><td>50.6</td><td><b>54.1</b></td><td><b>54.8 ± 0.1</b></td></tr>
      <tr><td>Medium</td><td>Hopper</td><td>58.0</td><td>54.5</td><td>66.3</td><td>57.1</td><td>79.3</td><td>58.5</td><td>82.4</td><td><b>98.0</b></td><td><b>99.5 ± 1.7</b></td></tr>
      <tr><td>Medium</td><td>Walker2d</td><td>79.2</td><td>53.1</td><td>78.3</td><td>77.9</td><td><b>82.5</b></td><td>79.7</td><td><b>85.1</b></td><td><b>86.0</b></td><td><b>86.5 ± 0.2</b></td></tr>
      <tr><td>Medium-Replay</td><td>HalfCheetah</td><td><b>46.2</b></td><td>38.2</td><td>44.2</td><td>37.1</td><td>39.3</td><td>42.2</td><td><b>47.5</b></td><td><b>47.6</b></td><td><b>47.8 ± 0.4</b></td></tr>
      <tr><td>Medium-Replay</td><td>Hopper</td><td>48.6</td><td>33.1</td><td>94.7</td><td>86.2</td><td><b>100.0</b></td><td><b>96.8</b></td><td><b>100.7</b></td><td><b>96.9</b></td><td><b>97.4 ± 4.0</b></td></tr>
      <tr><td>Medium-Replay</td><td>Walker2d</td><td>26.7</td><td>15.0</td><td>73.9</td><td>65.1</td><td>75.0</td><td>61.2</td><td><b>94.3</b></td><td>84.4</td><td>79.3 ± 9.7</td></tr>
      <tr><td colspan="2"><b>Average (Locomotion)</b></td><td>63.9</td><td>51.9</td><td>76.9</td><td>75.6</td><td>81.8</td><td>75.3</td><td>86.3</td><td><b>86.6</b></td><td>86.1</td></tr>
    </tbody>
  </table>
        </div>
        <h3>Image Generation</h3>
        <p>
      We evaluate our method on compositional text-to-image generation on <a href="https://github.com/Karine-Huang/T2I-CompBench">CompBench</a>, and class conditional ImageNet generation using an unconditional diffusion model with a pretrained classifier.
    </p>
    <p>
      In compositional test-to-image, we show that inference-time search against a pretrained visual verifier such as <a href="https://github.com/xingyizhou/UniDet">UniDet</a> and <a href="https://github.com/salesforce/BLIP">BLIP</a> can generate images with complex compositional prompts. 
    </p>
    <p>
      Below show the results of prompt "eight bottles" with and without inference scaling, which requires the model to generate the correct number of objects.
      We use a <strong>UniDet Model</strong> to count the number of objects in the generated image.
      After inference scaling the number of bottles in the generated image follows the prompt, while without inference scaling the number of bottles is not consistent with the prompt.
    </p>
    <div class="fig-row">
      <figure>
        <img src="t2i1.png" alt="wrong sample without inference scaling" />
        <figcaption>Generated sample without inference scaling. The number of bottles does not follow the prompt "eight bottles"</figcaption>
      </figure>
      <figure>
        <img src="t2i2.png" alt="correct sample with inference scaling" />
        <figcaption>Generated sample with inference scaling using UniDet. The number of bottles follows the prompt "eight bottles"</figcaption>
      </figure>
    </div>
    <p>
      Below is the scaling curve of DFS on the 5 datasets in the attribute binding and object relationship tasks.
    </p>
    <div class="fig-row">
      <figure>
        <img src="t2i_curve1.png" alt="scaling dfs attr" />
        <figcaption>Scaling curve of DFS in attribute binding task</figcaption>
      </figure>
      <figure>
        <img src="t2i_curve2.png" alt="scaling dfs obj" />
        <figcaption>Scaling curve of DFS in object relationship task</figcaption>
      </figure>
    </div>

      <p>
        In class-conditional image generation, we generate samples aligned with a target class label using an <strong>unconditional diffusion model</strong> guided by a pretrained ViT <strong>classifier</strong> as a verifier.
        $$\tilde{p}_0(\mathbf{x}_0) \propto p_0(\mathbf{x}_0) p\left(c|\mathbf{x}_0\right)$$
        Below, we show the uncurated samples for class 222 (“kuvasz”) on ImageNet, demonstrating diverse samples that accurately reflect the class label.
      </p>
      <figure>
        <img src="imagenet.png" alt="ImageNet class conditional generation", width="60%"/>
        <figcaption>
          Uncurated samples after inference-time scaling on ImageNet, with class 222 "kuvasz" 
      </figcaption>
      </figure>
      <p>
        Below is the scaling curve of BFS on ImageNet, showing FID and class accuracy over 256 samples.
      </p>
      <div class="fig-row">
      <figure>
        <img src="imagenet_fid.png" alt="scaling curve fid" />
        <figcaption>Scaling curve of BFS under FID metric</figcaption>
      </figure>
      <figure>
        <img src="imagenet_acc.png" alt="scaling curve acc" />
        <figcaption>Scaling curve of BFS under class accuracy metric</figcaption>
      </figure>
    </div>

      </section>
      <section>
        <h2>Conclusion</h2>
        <p>We propose a principled and general framework for inference-time scaling of diffusion models through search, which can be applied to various tasks such as planning, reinforcement learning and image generation. Still, inference-time base approaches require more hyper-parameter tuning than naive sampling, which we leave as a future work</p>
      </section>
      

      <section>
        <h2>Citation</h2>
        <div style="position: relative; margin-bottom: 2em;">
          <button id="copy-bibtex" style="position: absolute; top: 10px; right: 10px; padding: 4px 10px; font-size: 0.95em; cursor: pointer;">Copy</button>
          <pre id="bibtex-entry" style="padding-top: 2.5em; background: #f9f9f9; border-radius: 4px; font-size: 0.98em; overflow-x: auto;">
@inproceedings{your2024diffusionscaling,
  title={Inference-Time Scaling of Diffusion Models through Classical Search},
  author={Xiangcheng Zhang and Haowei Lin and Haotian Ye and James Zou and Jianzhu Ma and Yitao Liang and Yilun Du},
  booktitle={Conference on Example AI Research},
  year={2024}
}
          </pre>
        </div>
        <script>
          document.getElementById('copy-bibtex').onclick = function() {
            const bib = document.getElementById('bibtex-entry').innerText;
            navigator.clipboard.writeText(bib);
            this.textContent = 'Copied!';
            setTimeout(() => { this.textContent = 'Copy'; }, 1500);
          };
        </script>
      </section>




    </div>
  </main>
</body>
</html>